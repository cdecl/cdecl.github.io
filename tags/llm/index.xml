<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Llm on cdeclog</title><link>https://cdecl.github.io/tags/llm/</link><description>Recent content in Llm on cdeclog</description><generator>Hugo -- 0.157.0</generator><language>ko-kr</language><lastBuildDate>Wed, 25 Feb 2026 00:00:00 +0900</lastBuildDate><atom:link href="https://cdecl.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>2026 글로벌 LLM 생태계 비교: 파운데이션 모델부터 인프라 서비스까지</title><link>https://cdecl.github.io/devops/ai-models-comparison/</link><pubDate>Wed, 25 Feb 2026 00:00:00 +0900</pubDate><guid>https://cdecl.github.io/devops/ai-models-comparison/</guid><description>&lt;p&gt;2026년 현재 LLM 시장은 모델을 직접 학습시켜 배포하는 &lt;strong&gt;파운데이션 모델 제공자(Builders)&lt;/strong&gt; 와, 이 모델들을 기업 및 개발자가 쉽게 도입·최적화할 수 있도록 돕는 &lt;strong&gt;인프라 및 통합 서비스 제공자(Enablers)&lt;/strong&gt; 로 생태계가 양분되어 있습니다.&lt;/p&gt;
&lt;p&gt;북미의 프론티어 모델과 중국의 고효율 모델들이 치열하게 경쟁하는 가운데, Provider 관점에서 각 모델의 세부 특징과 인프라 서비스를 총정리합니다.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-파운데이션-모델-생태계-builders-북미-vs-중국"&gt;1. 파운데이션 모델 생태계 (Builders): 북미 vs 중국&lt;/h2&gt;
&lt;p&gt;LLM의 원천 지능을 제공하는 기업들입니다.&lt;br&gt;
북미는 범용적 에이전트와 멀티모달에, 중국은 극강의 가성비와 오픈소스 생태계에 집중하고 있습니다.&lt;/p&gt;</description></item><item><title>LLM 핵심 능력 해부: 추론 vs 코드 생성 vs Tool Calling, 그리고 자율 에이전트</title><link>https://cdecl.github.io/devops/llm-reasoning-vs-tool-calling/</link><pubDate>Wed, 25 Feb 2026 00:00:00 +0900</pubDate><guid>https://cdecl.github.io/devops/llm-reasoning-vs-tool-calling/</guid><description>&lt;p&gt;LLM은 모두 같은 방식으로 작동하는 것일까요? 최근 OpenAI o3, DeepSeek R1 같은 &lt;strong&gt;추론 모델(Reasoning Model)&lt;/strong&gt; 이 등장하면서, 기존 LLM과는 근본적으로 다른 사고 방식이 주목받고 있습니다. 또한 코드 생성이나 Tool Calling 능력은 추론과 어떤 관계에 있을까요?&lt;/p&gt;
&lt;p&gt;이 글에서는 LLM의 세 가지 핵심 능력 — &lt;strong&gt;일반 생성&lt;/strong&gt;, &lt;strong&gt;추론&lt;/strong&gt;, &lt;strong&gt;코드/Tool Calling&lt;/strong&gt; — 이 어떻게 다르고, 서로 어떤 관계를 갖는지 정리합니다.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-일반-llm-직감으로-답하기-system-1"&gt;1. 일반 LLM: &amp;ldquo;직감으로 답하기&amp;rdquo; (System 1)&lt;/h2&gt;
&lt;p&gt;일반 LLM(GPT-4o, Claude 3.5 Sonnet 등)은 본질적으로 &lt;strong&gt;다음 토큰 예측기(Next-Token Predictor)&lt;/strong&gt; 입니다. 학습된 수십억 개의 파라미터 속에서 패턴을 매칭하여, 입력에 가장 그럴듯한 다음 단어를 &lt;strong&gt;즉각적으로&lt;/strong&gt; 생성합니다.&lt;/p&gt;</description></item><item><title>AI Agent 구현의 두 갈래: 일반 Tool Calling vs MCP 비교</title><link>https://cdecl.github.io/devops/ai-agent-tool-calling-vs-mcp/</link><pubDate>Mon, 23 Feb 2026 00:00:00 +0900</pubDate><guid>https://cdecl.github.io/devops/ai-agent-tool-calling-vs-mcp/</guid><description>&lt;p&gt;AI 에이전트를 구축할 때, LLM이 외부 도구를 사용하게 만드는 과정은 필수적입니다. 하지만 최근 등장한 **MCP(Model Context Protocol)**와 기존의 &lt;strong&gt;Function/Tool Calling&lt;/strong&gt;은 비슷해 보이면서도 구조적으로 큰 차이가 있습니다. 오늘은 이 두 방식의 특징과 실제 구현 관점에서의 차이를 상세히 비교해 보겠습니다.&lt;/p&gt;
&lt;h2 id="1-한눈에-보는-비교-요약"&gt;1. 한눈에 보는 비교 요약&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left"&gt;구분&lt;/th&gt;
&lt;th style="text-align: left"&gt;일반 Tool Calling (기존 방식)&lt;/th&gt;
&lt;th style="text-align: left"&gt;MCP (Model Context Protocol)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left"&gt;&lt;strong&gt;핵심 개념&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left"&gt;함수 정의와 실행 로직의 수동 연결&lt;/td&gt;
&lt;td style="text-align: left"&gt;도구의 정의와 실행이 결합된 표준화된 서버&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left"&gt;&lt;strong&gt;실행 주체&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left"&gt;에이전트 애플리케이션 (Local, Tightly Coupled)&lt;/td&gt;
&lt;td style="text-align: left"&gt;독립된 MCP 서버 (Remote/Isolated)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left"&gt;&lt;strong&gt;통신 규격&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left"&gt;모델별 전용 API (OpenAI, Anthropic 등)&lt;/td&gt;
&lt;td style="text-align: left"&gt;&lt;strong&gt;JSON-RPC 2.0&lt;/strong&gt; 표준 프로토콜&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left"&gt;&lt;strong&gt;툴 목록 관리&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left"&gt;코드에 하드코딩, 앱 재배포 필요&lt;/td&gt;
&lt;td style="text-align: left"&gt;서버에서 동적으로 &lt;code&gt;list_tools()&lt;/code&gt; 조회&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left"&gt;&lt;strong&gt;확장성&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left"&gt;새 툴 추가 시 앱 코드 수정 및 재배포&lt;/td&gt;
&lt;td style="text-align: left"&gt;MCP 서버만 추가·재시작하면 즉시 연동&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left"&gt;&lt;strong&gt;상호운용성&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left"&gt;모델별 규격 변환 코드 직접 작성 필요&lt;/td&gt;
&lt;td style="text-align: left"&gt;MCP 지원 클라이언트라면 어떤 모델이든 재사용&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left"&gt;&lt;strong&gt;컨텍스트 제공&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left"&gt;주로 &amp;lsquo;액션(함수 호출)&amp;lsquo;에 집중&lt;/td&gt;
&lt;td style="text-align: left"&gt;툴 + &lt;strong&gt;리소스(파일, DB)&lt;/strong&gt; + &lt;strong&gt;프롬프트 템플릿&lt;/strong&gt; 패키지&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left"&gt;&lt;strong&gt;보안/격리&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left"&gt;에이전트 프로세스 내에서 직접 실행&lt;/td&gt;
&lt;td style="text-align: left"&gt;실행 로직이 서버에 캡슐화, 권한 경계 명확&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id="2-일반-tool-calling-직접-요리하기-방식"&gt;2. 일반 Tool Calling: &amp;ldquo;직접 요리하기&amp;rdquo; 방식&lt;/h2&gt;
&lt;p&gt;일반적인 방식에서 에이전트는 요리사(LLM)가 준 레시피(JSON)를 보고 **직접 요리(함수 실행)**를 합니다.&lt;br&gt;
실행 로직이 에이전트 코드 내부에 깊게 박혀 있는 구조(Tightly Coupled)입니다.&lt;/p&gt;</description></item><item><title>AI 에이전트(OpenClaw 등)의 LLM 인터페이스 구현 및 툴 콜링 기술 개요</title><link>https://cdecl.github.io/devops/ai-agent-openclaw-llm-interface/</link><pubDate>Sat, 21 Feb 2026 00:00:00 +0900</pubDate><guid>https://cdecl.github.io/devops/ai-agent-openclaw-llm-interface/</guid><description>&lt;p&gt;OpenClaw, Claude 데스크톱 앱, 혹은 로컬 기반의 여러 AI 에이전트들은 내부적으로 LLM(대형 언어 모델)과 어떻게 소통하고, 로컬 환경의 도구(Tool)들을 사용할까요? 이 글에서는 에이전트가 LLM과 인터페이스를 맺는 기술적 구현 내용과 핵심 요소들을 살펴봅니다.&lt;/p&gt;
&lt;h2 id="1-지침-파일agentmd-등-적용-방법"&gt;1. 지침 파일(&lt;code&gt;agent.md&lt;/code&gt; 등) 적용 방법&lt;/h2&gt;
&lt;p&gt;AI 에이전트의 페르소나, 역할, 기본 규칙을 정의하기 위해 주로 &lt;code&gt;.md&lt;/code&gt; 형태의 지침 파일을 사용합니다. (예: &lt;code&gt;agent.md&lt;/code&gt;, &lt;code&gt;system_prompt.txt&lt;/code&gt;, &lt;code&gt;SOUL.md&lt;/code&gt; 등)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;기술적 구현:&lt;/strong&gt;
이러한 지침 파일은 LLM에 전달되는 **시스템 프롬프트(System Prompt)**로 로드됩니다. 에이전트 프로그램이 실행될 때 혹은 세션이 시작될 때 파일 시스템에서 문서를 읽어 LLM의 &lt;code&gt;system&lt;/code&gt; 역할(role) 메시지에 주입합니다.&lt;/p&gt;</description></item><item><title>[AI 엔지니어링] 에이전트의 'Skills' 환상과 56%의 실패율: 왜 우리는 다시 시스템 프롬프트로 돌아가는가?</title><link>https://cdecl.github.io/posts/ai-agent-skills-vs-context/</link><pubDate>Fri, 30 Jan 2026 00:00:00 +0900</pubDate><guid>https://cdecl.github.io/posts/ai-agent-skills-vs-context/</guid><description>&lt;p&gt;최근 AI 개발자 커뮤니티, 특히 Vercel AI SDK와 Cursor 사용자들 사이에서 매우 흥미로운 화두가 던져졌습니다. Vercel의 소프트웨어 엔지니어 Jude Gao가 발표한 **&amp;quot;&lt;a href="https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals"&gt;AGENTS.md outperforms skills in our agent evals&lt;/a&gt;&amp;quot;**라는 벤치마크 결과입니다.&lt;/p&gt;
&lt;p&gt;많은 개발자가 프로젝트를 진행하며 직감적으로 느끼던 현상—&amp;ldquo;도구(Skills)를 쥐여주는 것보다, 그냥 문서를 통째로 읽게 시키는 게 훨씬 낫다&amp;rdquo;—가 실제 데이터로 증명되었습니다. 오늘은 이 벤치마크 데이터와 이를 둘러싼 &amp;lsquo;Skills vs Context vs Subagents&amp;rsquo; 아키텍처의 변화를 심도 있게 분석해 봅니다.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-충격적인-데이터-56의-무시율-ignore-rate"&gt;1. 충격적인 데이터: 56%의 무시율 (Ignore Rate)&lt;/h2&gt;
&lt;p&gt;우리는 흔히 &amp;ldquo;LLM에게 도구(Tool/Skill/Function Calling)를 주면, 필요할 때마다 똑똑하게 꺼내 쓸 것&amp;quot;이라고 기대합니다. 하지만 Next.js 16 API(당시 미학습 데이터)를 대상으로 한 벤치마크 결과는 이 믿음을 배신했습니다.&lt;/p&gt;</description></item><item><title>OpenRouter - 모든 AI 모델을 하나로 연결하는 관문</title><link>https://cdecl.github.io/devops/openrouter-ai-gateway-guide/</link><pubDate>Wed, 23 Jul 2025 00:00:00 +0900</pubDate><guid>https://cdecl.github.io/devops/openrouter-ai-gateway-guide/</guid><description>&lt;p&gt;수많은 대규모 언어 모델(LLM)이 등장하면서, 개발자와 기획자들은 프로젝트에 가장 적합한 모델을 선택하고 연동하는 데 많은 시간과 노력을 쏟고 있습니다. OpenRouter는 이러한 문제를 해결하기 위해 등장한 플랫폼으로, 다양한 AI 모델을 단일 API로 통합하여 제공하는 &amp;lsquo;AI의 관문(Gateway)&amp;rsquo; 역할을 합니다. 이 글에서는 OpenRouter의 핵심 기능부터 요금 체계, 활용 방법까지 자세히 알아보겠습니다.&lt;/p&gt;
&lt;h2 id="openrouter란"&gt;OpenRouter란?&lt;/h2&gt;
&lt;p&gt;OpenRouter(&lt;a href="https://openrouter.ai/"&gt;https://openrouter.ai/&lt;/a&gt;)는 GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, Llama 3.1 등 여러 제공업체의 최신 AI 모델들을 하나의 표준화된 API 엔드포인트로 묶어 제공하는 서비스입니다. 개발자는 더 이상 각 모델의 API 문서를 따로 학습하거나, 여러 결제 시스템을 관리할 필요 없이 OpenRouter를 통해 원하는 모델을 손쉽게 테스트하고 실제 프로덕션에 적용할 수 있습니다.&lt;/p&gt;</description></item><item><title>Ollama를 이용한 Mistral 로컬 실행 가이드</title><link>https://cdecl.github.io/devops/ollama-mistral/</link><pubDate>Sat, 27 Jan 2024 00:00:00 +0900</pubDate><guid>https://cdecl.github.io/devops/ollama-mistral/</guid><description>&lt;p&gt;ollama, ollama-webui, mistral 설치 및 테스트&lt;/p&gt;
&lt;h2 id="ollama"&gt;Ollama&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;OLLAMA는 Open Large Language Model for AI Applications의 약자로, Google AI에서 개발한 대규모 언어 모델 (LLM)입니다.
OLLAMA는 텍스트 생성, 번역, 질문 응답 등 다양한 AI 애플리케이션 개발을 위해 사용할 수 있는 강력한 도구&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;다양한 기능: OLLAMA는 텍스트 생성, 번역, 질문 응답, 요약, 코드 생성 등 다양한 기능을 제공합니다.&lt;/li&gt;
&lt;li&gt;강력한 성능: OLLAMA는 Google AI의 최첨단 기술을 기반으로 개발되어 강력한 성능을 제공합니다.&lt;/li&gt;
&lt;li&gt;쉬운 사용: OLLAMA는 Python API를 제공하여 쉽게 사용할 수 있습니다.&lt;/li&gt;
&lt;li&gt;다양한 모델: OLLAMA는 다양한 크기와 기능을 가진 모델을 제공하여 사용자의 필요에 맞게 선택할 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ollama-설치"&gt;Ollama 설치&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;다운로드 : &lt;a href="https://ollama.ai/download"&gt;https://ollama.ai/download&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;}&lt;/li&gt;
&lt;li&gt;설치 및 활용 가능한 모델 : &lt;a href="https://ollama.ai/library"&gt;https://ollama.ai/library&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;}&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;$ ollama
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Usage:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ollama &lt;span style="color:#ff79c6"&gt;[&lt;/span&gt;flags&lt;span style="color:#ff79c6"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ollama &lt;span style="color:#ff79c6"&gt;[&lt;/span&gt;command&lt;span style="color:#ff79c6"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Available Commands:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; serve Start ollama
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; create Create a model from a Modelfile
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; show Show information &lt;span style="color:#ff79c6"&gt;for&lt;/span&gt; a model
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; run Run a model
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; pull Pull a model from a registry
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; push Push a model to a registry
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; list List models
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; cp Copy a model
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; rm Remove a model
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#8be9fd;font-style:italic"&gt;help&lt;/span&gt; Help about any &lt;span style="color:#8be9fd;font-style:italic"&gt;command&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Flags:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; -h, --help &lt;span style="color:#8be9fd;font-style:italic"&gt;help&lt;/span&gt; &lt;span style="color:#ff79c6"&gt;for&lt;/span&gt; ollama
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; -v, --version Show version information
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Use &lt;span style="color:#f1fa8c"&gt;&amp;#34;ollama [command] --help&amp;#34;&lt;/span&gt; &lt;span style="color:#ff79c6"&gt;for&lt;/span&gt; more information about a command.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="mistral-모델-설치"&gt;mistral 모델 설치&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ollama.ai/library/mistral"&gt;https://ollama.ai/library/mistral&lt;/a&gt;{:target=&amp;quot;_blank&amp;quot;}&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Mistral은 최근에 개발된 대형 언어 모델 중 하나입니다.
이 모델은 7.3 billion 개의 파라미터를 가지고 있으며, 자연어 처리 분야에서 매우 높은 성능을 보입니다.
Mistral은 다양한 자연어 처리 작업에서 사용될 수 있습니다.
예를 들어, 이 모델은 텍스트 생성, 기계 번역, 질문 응답, 감성 분석 등의 작업에 사용될 수 있습니다.&lt;/p&gt;</description></item></channel></rss>