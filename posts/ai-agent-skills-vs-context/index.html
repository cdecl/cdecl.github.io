<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI 엔지니어링] 에이전트의 'Skills' 환상과 56%의 실패율: 왜 우리는 다시 시스템 프롬프트로 돌아가는가? | cdeclog</title><meta name=keywords content="AI,Agent,Vercel,LLM,DevOps,Engineering"><meta name=description content='최근 AI 개발자 커뮤니티, 특히 Vercel AI SDK와 Cursor 사용자들 사이에서 매우 흥미로운 화두가 던져졌습니다. Vercel의 소프트웨어 엔지니어 Jude Gao가 발표한 **"AGENTS.md outperforms skills in our agent evals"**라는 벤치마크 결과입니다.
많은 개발자가 프로젝트를 진행하며 직감적으로 느끼던 현상—&ldquo;도구(Skills)를 쥐여주는 것보다, 그냥 문서를 통째로 읽게 시키는 게 훨씬 낫다&rdquo;—가 실제 데이터로 증명되었습니다. 오늘은 이 벤치마크 데이터와 이를 둘러싼 &lsquo;Skills vs Context vs Subagents&rsquo; 아키텍처의 변화를 심도 있게 분석해 봅니다.

1. 충격적인 데이터: 56%의 무시율 (Ignore Rate)
우리는 흔히 &ldquo;LLM에게 도구(Tool/Skill/Function Calling)를 주면, 필요할 때마다 똑똑하게 꺼내 쓸 것"이라고 기대합니다. 하지만 Next.js 16 API(당시 미학습 데이터)를 대상으로 한 벤치마크 결과는 이 믿음을 배신했습니다.'><meta name=author content="Byung Kyu KIM"><link rel=canonical href=https://cdecl.github.io/posts/ai-agent-skills-vs-context/><link crossorigin=anonymous href=/assets/css/stylesheet.f939c4ffefb264e6fe85e04352266f79db6bb1303c8e16ae9b6064c1247b5e32.css integrity="sha256-+TnE/++yZOb+heBDUiZvedtrsTA8jhaum2BkwSR7XjI=" rel="preload stylesheet" as=style><link rel=icon href=https://cdecl.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://cdecl.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://cdecl.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://cdecl.github.io/apple-touch-icon.png><link rel=mask-icon href=https://cdecl.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://cdecl.github.io/posts/ai-agent-skills-vs-context/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-VQQHHYPN7K"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-VQQHHYPN7K")}</script><meta property="og:url" content="https://cdecl.github.io/posts/ai-agent-skills-vs-context/"><meta property="og:site_name" content="cdeclog"><meta property="og:title" content="[AI 엔지니어링] 에이전트의 'Skills' 환상과 56%의 실패율: 왜 우리는 다시 시스템 프롬프트로 돌아가는가?"><meta property="og:description" content='최근 AI 개발자 커뮤니티, 특히 Vercel AI SDK와 Cursor 사용자들 사이에서 매우 흥미로운 화두가 던져졌습니다. Vercel의 소프트웨어 엔지니어 Jude Gao가 발표한 **"AGENTS.md outperforms skills in our agent evals"**라는 벤치마크 결과입니다.
많은 개발자가 프로젝트를 진행하며 직감적으로 느끼던 현상—“도구(Skills)를 쥐여주는 것보다, 그냥 문서를 통째로 읽게 시키는 게 훨씬 낫다”—가 실제 데이터로 증명되었습니다. 오늘은 이 벤치마크 데이터와 이를 둘러싼 ‘Skills vs Context vs Subagents’ 아키텍처의 변화를 심도 있게 분석해 봅니다.
1. 충격적인 데이터: 56%의 무시율 (Ignore Rate) 우리는 흔히 “LLM에게 도구(Tool/Skill/Function Calling)를 주면, 필요할 때마다 똑똑하게 꺼내 쓸 것"이라고 기대합니다. 하지만 Next.js 16 API(당시 미학습 데이터)를 대상으로 한 벤치마크 결과는 이 믿음을 배신했습니다.'><meta property="og:locale" content="ko-kr"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-30T00:00:00+09:00"><meta property="article:modified_time" content="2026-01-30T00:00:00+09:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="Agent"><meta property="article:tag" content="Vercel"><meta property="article:tag" content="LLM"><meta property="article:tag" content="DevOps"><meta property="article:tag" content="Engineering"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI 엔지니어링] 에이전트의 'Skills' 환상과 56%의 실패율: 왜 우리는 다시 시스템 프롬프트로 돌아가는가?"><meta name=twitter:description content='최근 AI 개발자 커뮤니티, 특히 Vercel AI SDK와 Cursor 사용자들 사이에서 매우 흥미로운 화두가 던져졌습니다. Vercel의 소프트웨어 엔지니어 Jude Gao가 발표한 **"AGENTS.md outperforms skills in our agent evals"**라는 벤치마크 결과입니다.
많은 개발자가 프로젝트를 진행하며 직감적으로 느끼던 현상—&ldquo;도구(Skills)를 쥐여주는 것보다, 그냥 문서를 통째로 읽게 시키는 게 훨씬 낫다&rdquo;—가 실제 데이터로 증명되었습니다. 오늘은 이 벤치마크 데이터와 이를 둘러싼 &lsquo;Skills vs Context vs Subagents&rsquo; 아키텍처의 변화를 심도 있게 분석해 봅니다.

1. 충격적인 데이터: 56%의 무시율 (Ignore Rate)
우리는 흔히 &ldquo;LLM에게 도구(Tool/Skill/Function Calling)를 주면, 필요할 때마다 똑똑하게 꺼내 쓸 것"이라고 기대합니다. 하지만 Next.js 16 API(당시 미학습 데이터)를 대상으로 한 벤치마크 결과는 이 믿음을 배신했습니다.'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cdecl.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[AI 엔지니어링] 에이전트의 'Skills' 환상과 56%의 실패율: 왜 우리는 다시 시스템 프롬프트로 돌아가는가?","item":"https://cdecl.github.io/posts/ai-agent-skills-vs-context/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI 엔지니어링] 에이전트의 'Skills' 환상과 56%의 실패율: 왜 우리는 다시 시스템 프롬프트로 돌아가는가?","name":"[AI 엔지니어링] 에이전트의 \u0027Skills\u0027 환상과 56%의 실패율: 왜 우리는 다시 시스템 프롬프트로 돌아가는가?","description":"최근 AI 개발자 커뮤니티, 특히 Vercel AI SDK와 Cursor 사용자들 사이에서 매우 흥미로운 화두가 던져졌습니다. Vercel의 소프트웨어 엔지니어 Jude Gao가 발표한 **\u0026quot;AGENTS.md outperforms skills in our agent evals\u0026quot;**라는 벤치마크 결과입니다.\n많은 개발자가 프로젝트를 진행하며 직감적으로 느끼던 현상—\u0026ldquo;도구(Skills)를 쥐여주는 것보다, 그냥 문서를 통째로 읽게 시키는 게 훨씬 낫다\u0026rdquo;—가 실제 데이터로 증명되었습니다. 오늘은 이 벤치마크 데이터와 이를 둘러싼 \u0026lsquo;Skills vs Context vs Subagents\u0026rsquo; 아키텍처의 변화를 심도 있게 분석해 봅니다.\n1. 충격적인 데이터: 56%의 무시율 (Ignore Rate) 우리는 흔히 \u0026ldquo;LLM에게 도구(Tool/Skill/Function Calling)를 주면, 필요할 때마다 똑똑하게 꺼내 쓸 것\u0026quot;이라고 기대합니다. 하지만 Next.js 16 API(당시 미학습 데이터)를 대상으로 한 벤치마크 결과는 이 믿음을 배신했습니다.\n","keywords":["AI","Agent","Vercel","LLM","DevOps","Engineering"],"articleBody":"최근 AI 개발자 커뮤니티, 특히 Vercel AI SDK와 Cursor 사용자들 사이에서 매우 흥미로운 화두가 던져졌습니다. Vercel의 소프트웨어 엔지니어 Jude Gao가 발표한 **\"AGENTS.md outperforms skills in our agent evals\"**라는 벤치마크 결과입니다.\n많은 개발자가 프로젝트를 진행하며 직감적으로 느끼던 현상—“도구(Skills)를 쥐여주는 것보다, 그냥 문서를 통째로 읽게 시키는 게 훨씬 낫다”—가 실제 데이터로 증명되었습니다. 오늘은 이 벤치마크 데이터와 이를 둘러싼 ‘Skills vs Context vs Subagents’ 아키텍처의 변화를 심도 있게 분석해 봅니다.\n1. 충격적인 데이터: 56%의 무시율 (Ignore Rate) 우리는 흔히 “LLM에게 도구(Tool/Skill/Function Calling)를 주면, 필요할 때마다 똑똑하게 꺼내 쓸 것\"이라고 기대합니다. 하지만 Next.js 16 API(당시 미학습 데이터)를 대상으로 한 벤치마크 결과는 이 믿음을 배신했습니다.\nSkills 방식의 현실 56%의 무시율: 에이전트에게 관련 문서를 도구 형태로 제공했을 때, 모델이 이를 호출하지 않고 자신의 낡은 내부 지식(Internal Weights)으로 대충 답변하거나 환각(Hallucination)을 일으킨 비율이 **절반(56%)**을 넘었습니다. 원인 (LLM의 나태함): LLM은 기본적으로 ‘게으른(Lazy)’ 성향을 가집니다. 추론 과정에서 토큰과 에너지를 절약하려다 보니, 확실하지 않은 상황에서 도구 호출이라는 복잡한 절차를 건너뛰는 경향이 있습니다. 시스템 프롬프트(Context)의 승리 반면, 동일한 문서를 AGENTS.md와 같은 파일에 담아 시스템 프롬프트에 강제로 주입(Context Injection) 했을 때의 결과는 놀라웠습니다.\n정답률 100%: 단 8KB로 압축된 문서를 시스템 프롬프트로 강제 제공했을 때, 테스트 케이스에서 완벽한 성능을 보였습니다. 모델에게 “선택\"을 맡기지 않고 “이 지식을 베이스로 하라\"고 강제했기 때문입니다. [!IMPORTANT] ‘자율성(Agentic)‘보다 ‘확정성(Deterministic)‘이 필요한 지식 참조 업무에서는 강제 컨텍스트 주입이 정답입니다.\n2. ‘전지적 시야’ vs ‘터널 시야’ 도구를 강제로 사용하게 유도하더라도(이 경우 정답률 79%), 왜 컨텍스트 주입(100%)을 이기지 못할까요? Vercel은 이를 시야의 차이로 설명합니다.\nContext 주입 (전지적 시야): 모델은 문서 전체의 맥락과 뉘앙스, 파일 간의 유기적 관계를 통째로 이해합니다. Skills 방식 (터널 시야): 검색된 일부 조각(Chunk)만 모델에게 전달됩니다. 파편화된 정보만으로는 복잡한 설계 규칙을 완벽히 준수하기 어렵습니다. 또한 속도 측면에서도 차이가 큽니다.\nSkills: 질문 → 생각 → 도구 호출 → 실행 → 결과 수신 → 최종 답변 (최소 2회 이상의 추론 필요) Context: 질문 + 문서 → 최종 답변 (단 1회의 추론으로 종결) 3. Context Injection vs Subagent: 무엇이 다른가? 최근에는 Skills의 대안으로 Subagent(하위 에이전트) 방식도 활발히 논의됩니다.\n비교 항목 Context Injection (단일 천재) Subagent (전문가 팀) 비유 매뉴얼을 전부 암기한 한 명의 천재 코딩, QA, 문서화 전문가로 구성된 팀 핵심 강점 통합적 사고: A파일 규칙과 B파일 규칙의 모순을 찾아냄. 격리된 전문성: 맥락 오염(Context Pollution) 방지 및 전문 업무 수행. 비용/속도 저렴하고 빠름 (1회 호출) 비싸고 느림 (N회 호출 + 통신 비용) 적합한 업무 지식 조회, 규칙 준수, 간단한 코드 리뷰 복잡한 워크플로우 실행, TDD, 단계별 과제 수행 4. 결론: 2026년의 AI 아키텍처 제언 Vercel의 데이터와 현업의 경험을 종합해 볼 때, 우리는 **“무조건적인 에이전트화”**를 경계해야 합니다.\n현대적 AI 개발을 위한 3계명 지식(Knowledge)은 Context로 해결하라: 프로젝트의 규칙, 컨벤션, 핵심 문서는 Skills로 만들지 말고 시스템 프롬프트(AGENTS.md, .cursorrules)에 넣어라. LLM의 거대한 Context Window를 믿는 것이 가장 효율적이다. 행동(Action)만 Skill로 남겨라: 계산기, API 호출, DB 쓰기 작업 등 LLM이 직접 할 수 없는 ‘기능’만 도구로 제공하라. 복잡도는 Subagent로 분리하라: Context가 너무 비대해져서 모델이 헷갈리기 시작하거나(Context Pollution), 작업 단계가 너무 복잡할 때만 Subagent 패턴을 도입하라. 요약하자면:\n“모델에게 **선택권(Skill)**을 주면 게으름을 피우지만, **맥락(Context)**을 주면 천재가 되고, **역할(Subagent)**을 나누면 전문가가 된다.”\n부록: AI 도구별 에이전트 규칙 파일(System Prompt) 현황 각 도구는 시스템 프롬프트에 컨텍스트를 주입하기 위해 고유한 규칙 파일 명칭을 사용합니다. 이를 통해 지식을 ‘강제 주입’하여 정답률을 높일 수 있습니다.\n도구/환경 규칙 파일 명칭 특징 및 비고 Cursor .cursorrules / .cursor/rules/*.md 최신 버전은 폴더 방식(.cursor/rules/)을 통한 세밀한 규칙 관리 지원. Windsurf .windsurfrules 프로젝트 루트에서 전역적인 코딩 컨벤션 강제. Cline / Roo Code .clinerules 에이전트의 자율 행동 및 MCP 도구 사용 가이드라인 정의. Claude CLAUDE.md 프로젝트 루트에서 가이드라인, 코딩 스타일, 자주 사용하는 명령어 정의. Gemini ~/.gemini/GEMINI.md 사용자 정보 및 전역적인 컨텍스트(Memory)를 관리하는 핵심 파일. Antigravity brain/task.md Brain 아키텍처의 핵심. 현재 작업 상태와 구현 계획을 담은 동적 컨텍스트. GitHub Copilot .github/copilot-instructions.md 코파일럿의 답변 스타일 및 특정 프레임워크 사용 지침 설정. Vercel AI SDK AGENTS.md 벤치마크에서 성능이 증명된 방식. 대규모 문서를 요약 및 인덱싱하여 주입. 출처: Vercel Blog - AGENTS.md outperforms skills in our agent evals\n","wordCount":"622","inLanguage":"en","datePublished":"2026-01-30T00:00:00+09:00","dateModified":"2026-01-30T00:00:00+09:00","author":{"@type":"Person","name":"Byung Kyu KIM"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cdecl.github.io/posts/ai-agent-skills-vs-context/"},"publisher":{"@type":"Organization","name":"cdeclog","logo":{"@type":"ImageObject","url":"https://cdecl.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://cdecl.github.io/ accesskey=h title="cdeclog (Alt + H)">cdeclog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://cdecl.github.io/dev/ title=Dev><span>Dev</span></a></li><li><a href=https://cdecl.github.io/devops/ title=DevOps><span>DevOps</span></a></li><li><a href=https://cdecl.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://cdecl.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://cdecl.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://cdecl.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://cdecl.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">[AI 엔지니어링] 에이전트의 'Skills' 환상과 56%의 실패율: 왜 우리는 다시 시스템 프롬프트로 돌아가는가?</h1><div class=post-meta><span title='2026-01-30 00:00:00 +0900 KST'>January 30, 2026</span>&nbsp;·&nbsp;<span>Byung Kyu KIM</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-%ec%b6%a9%ea%b2%a9%ec%a0%81%ec%9d%b8-%eb%8d%b0%ec%9d%b4%ed%84%b0-56%ec%9d%98-%eb%ac%b4%ec%8b%9c%ec%9c%a8-ignore-rate aria-label="1. 충격적인 데이터: 56%의 무시율 (Ignore Rate)">1. 충격적인 데이터: 56%의 무시율 (Ignore Rate)</a><ul><li><a href=#skills-%eb%b0%a9%ec%8b%9d%ec%9d%98-%ed%98%84%ec%8b%a4 aria-label="Skills 방식의 현실">Skills 방식의 현실</a></li><li><a href=#%ec%8b%9c%ec%8a%a4%ed%85%9c-%ed%94%84%eb%a1%ac%ed%94%84%ed%8a%b8context%ec%9d%98-%ec%8a%b9%eb%a6%ac aria-label="시스템 프롬프트(Context)의 승리">시스템 프롬프트(Context)의 승리</a></li></ul></li><li><a href=#2-%ec%a0%84%ec%a7%80%ec%a0%81-%ec%8b%9c%ec%95%bc-vs-%ed%84%b0%eb%84%90-%ec%8b%9c%ec%95%bc aria-label="2. &lsquo;전지적 시야&rsquo; vs &lsquo;터널 시야&rsquo;">2. &lsquo;전지적 시야&rsquo; vs &lsquo;터널 시야&rsquo;</a></li><li><a href=#3-context-injection-vs-subagent-%eb%ac%b4%ec%97%87%ec%9d%b4-%eb%8b%a4%eb%a5%b8%ea%b0%80 aria-label="3. Context Injection vs Subagent: 무엇이 다른가?">3. Context Injection vs Subagent: 무엇이 다른가?</a></li><li><a href=#4-%ea%b2%b0%eb%a1%a0-2026%eb%85%84%ec%9d%98-ai-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98-%ec%a0%9c%ec%96%b8 aria-label="4. 결론: 2026년의 AI 아키텍처 제언">4. 결론: 2026년의 AI 아키텍처 제언</a><ul><li><a href=#%ed%98%84%eb%8c%80%ec%a0%81-ai-%ea%b0%9c%eb%b0%9c%ec%9d%84-%ec%9c%84%ed%95%9c-3%ea%b3%84%eb%aa%85 aria-label="현대적 AI 개발을 위한 3계명">현대적 AI 개발을 위한 3계명</a></li></ul></li><li><a href=#%eb%b6%80%eb%a1%9d-ai-%eb%8f%84%ea%b5%ac%eb%b3%84-%ec%97%90%ec%9d%b4%ec%a0%84%ed%8a%b8-%ea%b7%9c%ec%b9%99-%ed%8c%8c%ec%9d%bcsystem-prompt-%ed%98%84%ed%99%a9 aria-label="부록: AI 도구별 에이전트 규칙 파일(System Prompt) 현황">부록: AI 도구별 에이전트 규칙 파일(System Prompt) 현황</a></li></ul></div></details></div><div class=post-content><p>최근 AI 개발자 커뮤니티, 특히 Vercel AI SDK와 Cursor 사용자들 사이에서 매우 흥미로운 화두가 던져졌습니다. Vercel의 소프트웨어 엔지니어 Jude Gao가 발표한 **"<a href=https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals>AGENTS.md outperforms skills in our agent evals</a>"**라는 벤치마크 결과입니다.</p><p>많은 개발자가 프로젝트를 진행하며 직감적으로 느끼던 현상—&ldquo;도구(Skills)를 쥐여주는 것보다, 그냥 문서를 통째로 읽게 시키는 게 훨씬 낫다&rdquo;—가 실제 데이터로 증명되었습니다. 오늘은 이 벤치마크 데이터와 이를 둘러싼 &lsquo;Skills vs Context vs Subagents&rsquo; 아키텍처의 변화를 심도 있게 분석해 봅니다.</p><hr><h2 id=1-충격적인-데이터-56의-무시율-ignore-rate>1. 충격적인 데이터: 56%의 무시율 (Ignore Rate)<a hidden class=anchor aria-hidden=true href=#1-충격적인-데이터-56의-무시율-ignore-rate>#</a></h2><p>우리는 흔히 &ldquo;LLM에게 도구(Tool/Skill/Function Calling)를 주면, 필요할 때마다 똑똑하게 꺼내 쓸 것"이라고 기대합니다. 하지만 Next.js 16 API(당시 미학습 데이터)를 대상으로 한 벤치마크 결과는 이 믿음을 배신했습니다.</p><h3 id=skills-방식의-현실>Skills 방식의 현실<a hidden class=anchor aria-hidden=true href=#skills-방식의-현실>#</a></h3><ul><li><strong>56%의 무시율:</strong> 에이전트에게 관련 문서를 도구 형태로 제공했을 때, 모델이 이를 호출하지 않고 자신의 낡은 내부 지식(Internal Weights)으로 대충 답변하거나 환각(Hallucination)을 일으킨 비율이 **절반(56%)**을 넘었습니다.</li><li><strong>원인 (LLM의 나태함):</strong> LLM은 기본적으로 &lsquo;게으른(Lazy)&rsquo; 성향을 가집니다. 추론 과정에서 토큰과 에너지를 절약하려다 보니, 확실하지 않은 상황에서 도구 호출이라는 복잡한 절차를 건너뛰는 경향이 있습니다.</li></ul><h3 id=시스템-프롬프트context의-승리>시스템 프롬프트(Context)의 승리<a hidden class=anchor aria-hidden=true href=#시스템-프롬프트context의-승리>#</a></h3><p>반면, 동일한 문서를 <code>AGENTS.md</code>와 같은 파일에 담아 <strong>시스템 프롬프트에 강제로 주입(Context Injection)</strong> 했을 때의 결과는 놀라웠습니다.</p><ul><li><strong>정답률 100%:</strong> 단 8KB로 압축된 문서를 시스템 프롬프트로 강제 제공했을 때, 테스트 케이스에서 <strong>완벽한 성능</strong>을 보였습니다. 모델에게 &ldquo;선택"을 맡기지 않고 &ldquo;이 지식을 베이스로 하라"고 강제했기 때문입니다.</li></ul><blockquote><p>[!IMPORTANT]
&lsquo;자율성(Agentic)&lsquo;보다 &lsquo;확정성(Deterministic)&lsquo;이 필요한 지식 참조 업무에서는 <strong>강제 컨텍스트 주입</strong>이 정답입니다.</p></blockquote><hr><h2 id=2-전지적-시야-vs-터널-시야>2. &lsquo;전지적 시야&rsquo; vs &lsquo;터널 시야&rsquo;<a hidden class=anchor aria-hidden=true href=#2-전지적-시야-vs-터널-시야>#</a></h2><p>도구를 강제로 사용하게 유도하더라도(이 경우 정답률 79%), 왜 컨텍스트 주입(100%)을 이기지 못할까요? Vercel은 이를 시야의 차이로 설명합니다.</p><ul><li><strong>Context 주입 (전지적 시야):</strong> 모델은 문서 전체의 맥락과 뉘앙스, 파일 간의 유기적 관계를 통째로 이해합니다.</li><li><strong>Skills 방식 (터널 시야):</strong> 검색된 일부 조각(Chunk)만 모델에게 전달됩니다. 파편화된 정보만으로는 복잡한 설계 규칙을 완벽히 준수하기 어렵습니다.</li></ul><p>또한 <strong>속도</strong> 측면에서도 차이가 큽니다.</p><ul><li><strong>Skills:</strong> <code>질문</code> → <code>생각</code> → <code>도구 호출</code> → <code>실행</code> → <code>결과 수신</code> → <code>최종 답변</code> (최소 2회 이상의 추론 필요)</li><li><strong>Context:</strong> <code>질문 + 문서</code> → <code>최종 답변</code> (단 1회의 추론으로 종결)</li></ul><hr><h2 id=3-context-injection-vs-subagent-무엇이-다른가>3. Context Injection vs Subagent: 무엇이 다른가?<a hidden class=anchor aria-hidden=true href=#3-context-injection-vs-subagent-무엇이-다른가>#</a></h2><p>최근에는 Skills의 대안으로 <strong>Subagent(하위 에이전트)</strong> 방식도 활발히 논의됩니다.</p><table><thead><tr><th style=text-align:left>비교 항목</th><th style=text-align:left>Context Injection (단일 천재)</th><th style=text-align:left>Subagent (전문가 팀)</th></tr></thead><tbody><tr><td style=text-align:left><strong>비유</strong></td><td style=text-align:left>매뉴얼을 전부 암기한 한 명의 천재</td><td style=text-align:left>코딩, QA, 문서화 전문가로 구성된 팀</td></tr><tr><td style=text-align:left><strong>핵심 강점</strong></td><td style=text-align:left><strong>통합적 사고:</strong> A파일 규칙과 B파일 규칙의 모순을 찾아냄.</td><td style=text-align:left><strong>격리된 전문성:</strong> 맥락 오염(Context Pollution) 방지 및 전문 업무 수행.</td></tr><tr><td style=text-align:left><strong>비용/속도</strong></td><td style=text-align:left>저렴하고 빠름 (1회 호출)</td><td style=text-align:left>비싸고 느림 (N회 호출 + 통신 비용)</td></tr><tr><td style=text-align:left><strong>적합한 업무</strong></td><td style=text-align:left>지식 조회, 규칙 준수, 간단한 코드 리뷰</td><td style=text-align:left>복잡한 워크플로우 실행, TDD, 단계별 과제 수행</td></tr></tbody></table><hr><h2 id=4-결론-2026년의-ai-아키텍처-제언>4. 결론: 2026년의 AI 아키텍처 제언<a hidden class=anchor aria-hidden=true href=#4-결론-2026년의-ai-아키텍처-제언>#</a></h2><p>Vercel의 데이터와 현업의 경험을 종합해 볼 때, 우리는 **&ldquo;무조건적인 에이전트화&rdquo;**를 경계해야 합니다.</p><h3 id=현대적-ai-개발을-위한-3계명>현대적 AI 개발을 위한 3계명<a hidden class=anchor aria-hidden=true href=#현대적-ai-개발을-위한-3계명>#</a></h3><ol><li><strong>지식(Knowledge)은 Context로 해결하라:</strong> 프로젝트의 규칙, 컨벤션, 핵심 문서는 Skills로 만들지 말고 시스템 프롬프트(<code>AGENTS.md</code>, <code>.cursorrules</code>)에 넣어라. LLM의 거대한 Context Window를 믿는 것이 가장 효율적이다.</li><li><strong>행동(Action)만 Skill로 남겨라:</strong> 계산기, API 호출, DB 쓰기 작업 등 LLM이 직접 할 수 없는 &lsquo;기능&rsquo;만 도구로 제공하라.</li><li><strong>복잡도는 Subagent로 분리하라:</strong> Context가 너무 비대해져서 모델이 헷갈리기 시작하거나(Context Pollution), 작업 단계가 너무 복잡할 때만 Subagent 패턴을 도입하라.</li></ol><p><strong>요약하자면:</strong></p><blockquote><p>&ldquo;모델에게 **선택권(Skill)**을 주면 게으름을 피우지만, **맥락(Context)**을 주면 천재가 되고, **역할(Subagent)**을 나누면 전문가가 된다.&rdquo;</p></blockquote><hr><h2 id=부록-ai-도구별-에이전트-규칙-파일system-prompt-현황>부록: AI 도구별 에이전트 규칙 파일(System Prompt) 현황<a hidden class=anchor aria-hidden=true href=#부록-ai-도구별-에이전트-규칙-파일system-prompt-현황>#</a></h2><p>각 도구는 시스템 프롬프트에 컨텍스트를 주입하기 위해 고유한 규칙 파일 명칭을 사용합니다. 이를 통해 지식을 &lsquo;강제 주입&rsquo;하여 정답률을 높일 수 있습니다.</p><table><thead><tr><th style=text-align:left>도구/환경</th><th style=text-align:left>규칙 파일 명칭</th><th style=text-align:left>특징 및 비고</th></tr></thead><tbody><tr><td style=text-align:left><strong>Cursor</strong></td><td style=text-align:left><code>.cursorrules</code> / <code>.cursor/rules/*.md</code></td><td style=text-align:left>최신 버전은 폴더 방식(<code>.cursor/rules/</code>)을 통한 세밀한 규칙 관리 지원.</td></tr><tr><td style=text-align:left><strong>Windsurf</strong></td><td style=text-align:left><code>.windsurfrules</code></td><td style=text-align:left>프로젝트 루트에서 전역적인 코딩 컨벤션 강제.</td></tr><tr><td style=text-align:left><strong>Cline / Roo Code</strong></td><td style=text-align:left><code>.clinerules</code></td><td style=text-align:left>에이전트의 자율 행동 및 MCP 도구 사용 가이드라인 정의.</td></tr><tr><td style=text-align:left><strong>Claude</strong></td><td style=text-align:left><code>CLAUDE.md</code></td><td style=text-align:left>프로젝트 루트에서 가이드라인, 코딩 스타일, 자주 사용하는 명령어 정의.</td></tr><tr><td style=text-align:left><strong>Gemini</strong></td><td style=text-align:left><code>~/.gemini/GEMINI.md</code></td><td style=text-align:left>사용자 정보 및 전역적인 컨텍스트(Memory)를 관리하는 핵심 파일.</td></tr><tr><td style=text-align:left><strong>Antigravity</strong></td><td style=text-align:left><code>brain/task.md</code></td><td style=text-align:left><strong>Brain</strong> 아키텍처의 핵심. 현재 작업 상태와 구현 계획을 담은 동적 컨텍스트.</td></tr><tr><td style=text-align:left><strong>GitHub Copilot</strong></td><td style=text-align:left><code>.github/copilot-instructions.md</code></td><td style=text-align:left>코파일럿의 답변 스타일 및 특정 프레임워크 사용 지침 설정.</td></tr><tr><td style=text-align:left><strong>Vercel AI SDK</strong></td><td style=text-align:left><code>AGENTS.md</code></td><td style=text-align:left>벤치마크에서 성능이 증명된 방식. 대규모 문서를 요약 및 인덱싱하여 주입.</td></tr></tbody></table><hr><p><strong>출처:</strong> <a href=https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals>Vercel Blog - AGENTS.md outperforms skills in our agent evals</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://cdecl.github.io/tags/ai/>AI</a></li><li><a href=https://cdecl.github.io/tags/agent/>Agent</a></li><li><a href=https://cdecl.github.io/tags/vercel/>Vercel</a></li><li><a href=https://cdecl.github.io/tags/llm/>LLM</a></li><li><a href=https://cdecl.github.io/tags/devops/>DevOps</a></li><li><a href=https://cdecl.github.io/tags/engineering/>Engineering</a></li></ul></footer><script src=https://giscus.app/client.js data-repo=cdecl/cdecl.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkzNDk1ODUyNjg=" data-category=General data-category-id=DIC_kwDOFNY_dM4C1XMk data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=transparent_dark data-lang=ko crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2026 <a href=https://cdecl.github.io/>cdeclog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>